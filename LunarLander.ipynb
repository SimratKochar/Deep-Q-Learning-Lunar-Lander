{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning for Lunar Landing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 - Installing the required packages and importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in ./anaconda3/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./anaconda3/lib/python3.11/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./anaconda3/lib/python3.11/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./anaconda3/lib/python3.11/site-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./anaconda3/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in ./anaconda3/lib/python3.11/site-packages (1.1.1)\n",
      "\u001b[33mWARNING: gymnasium 1.1.1 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in ./anaconda3/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./anaconda3/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./anaconda3/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./anaconda3/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: ale_py>=0.9 in ./anaconda3/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (0.10.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: swig in ./anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting box2d\n",
      "  Downloading Box2D-2.3.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (573 bytes)\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Downloading Box2D-2.3.10-cp311-cp311-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygame-2.6.1-cp311-cp311-macosx_11_0_arm64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: box2d, pygame\n",
      "Successfully installed box2d-2.3.10 pygame-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium\n",
    "%pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "%pip install swig\n",
    "%pip install box2d pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Building the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed = 42) -> None:\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)            # first full connection between input layer and first fully connected layer\n",
    "        self.fc2 = nn.Linear(64, 64)                    # second full connection                       \n",
    "        self.fc3 = nn.Linear(64, action_size)           # second fully connected layer to output (what action should be taken)\n",
    "\n",
    "    def forward(self, state):        \n",
    "        x = self.fc1(state)         # propagate signal from the input layer to the first fully\n",
    "        x = F.relu(x)               # connected layer with a rectifier activation function\n",
    "        \n",
    "        x = self.fc2(x)             # propagate signal from the first fully connected layer to the second fully connected layer\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return self.fc3(x)          # return the output layer containting the possible actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Training the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "State size:  8\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('LunarLander-v3')\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "number_actions = env.action_space.n\n",
    "print('State shape: ', state_shape)\n",
    "print('State size: ', state_size)\n",
    "print('Number of actions: ', number_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4            # Why? Trial and error experimentation\n",
    "minibatch_size = 100            # Common practice for deep q-learning\n",
    "discount_factor = 0.99          # Close to 1, so that the agent considers future rewards instead of being shortsighted\n",
    "replay_buffer_size = int(1e5)   # Memory of the agent (How many experiences, states, rewards etc are stored in the agent)\n",
    "interpolation_param = 1e-3      # Again, experimentally determined to be optimal **Look up how it is determined**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    # Method to add experiences to the replay memory buffer, while checking that the memory is not exceeded\n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "            # Note to self: I think using linked lists instead of a list for memory would be a lot more efficient here... look it up\n",
    "\n",
    "    # Method to randomly select/sample a batch of experiences from the replay memory buffer\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, k = batch_size)\n",
    "        states_np = np.vstack([e[0] for e in experiences if e is not None])         # Stack all the states by extracting them from the experiences\n",
    "        states = torch.from_numpy(states_np).float().to(self.device)                # Creating a pytorch Tensor from the states numpy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the DQN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3280684268.py, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 46\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "        self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
    "        self.memory = ReplayMemory(replay_buffer_size)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory.memory) > minibatch_size:\n",
    "                experiences = self.memory.sample(100)\n",
    "                self.learn(experiences, discount_factor)\n",
    "\n",
    "    def act(self, state, epsilon = 0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.local_qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_qnetwork(state)\n",
    "        self.local_qnetwork.train()\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, discount_factor):\n",
    "        states, next_states, actions, rewards, dones = experiences\n",
    "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0]\n",
    "        q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
    "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward() #back-propagation\n",
    "        self.optimizer.step() #Perform the optimization step to update the model parameters\n",
    "        # Modify the target network parameters \n",
    "        self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_param) \n",
    "\n",
    "        # Softly update the target parameters with the weighted average of the local parameters and target parameters\n",
    "        def soft_update(self, local_model, target_model, interpolation_parameter):\n",
    "            for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "                target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)\n",
    "        \n",
    "        # AGENT CLASS IMPLEMENTED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode = 'rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action.item())\n",
    "    env.close()\n",
    "    imageio.mimsave('video.mp4', frames, fps = 30)\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
